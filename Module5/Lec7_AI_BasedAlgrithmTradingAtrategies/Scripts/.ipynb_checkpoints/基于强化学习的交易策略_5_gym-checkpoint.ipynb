{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff44e58",
   "metadata": {},
   "source": [
    "\n",
    "# 5. 强化学习平台\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e58d146",
   "metadata": {},
   "source": [
    "在第三部分中，我们使用手动编程的方法（hard coding）来编写了一个简单的增强学习处理迷宫问题的案例。真实情况下，我们遇到的问题更加复杂，需要将程序编写过程进行简化，将更多的注意力放到问题的解决方案上来。机器学习的一个定义就是不需要用显著编程的方式来实现运算过程，由此我们需要强化学习的平台，以便代码结构更加简单。\n",
    "\n",
    "对于强化学习，有以下平台可供使用：\n",
    "\n",
    "- OpenAI Gym是一个用于构建、评估和比较强化学习算法的工具包，能够兼容TensorFlow、Theano、Keras等任何框架下编写的算法，该工具包简单易懂，无需对智能体的结构进行任何假设，并对所有强化学习任务提供了接口。更多信息参看[帮助文档](http://gym.openai.com/)。\n",
    "\n",
    "\n",
    "- OpenAI Universe是OpenAI Gym的扩展，提供了从简单到实时复杂的各种环境下训练和评估智能体的功能，可以无限访问许多游戏环境。利用Universe，任何程序都可以转换为一个Gym环境，而无需访问程序内部、源代码或者API，因为Universe是通过一个计算远程桌面的虚拟网络来自动启动程序的。\n",
    "\n",
    "\n",
    "- DeepMind Lab是基于AI智能体的另一个优秀的研究平台，提供了丰富的模拟环境，可以作为运行多种强化学习算法的实验平台，同时它具有高度可定制化和可扩展性，可视化内容非常丰富，且具有科幻风格和逼真效果。\n",
    "\n",
    "\n",
    "- RL-Glue提供了连接智能体、环境和程序的接口，即使这些都是采用不同编程语言写的也可以。另外还可以在完成任务时与他人共享智能体和环境，由于具有兼容性，因此大大提高了可重用性。\n",
    "\n",
    "\n",
    "- Project Malmo是微软在Minecraft基础上开发的另一种AI实验平台，可为定制化环境提供良好的灵活性，同时还适用于复杂环境。另外，还允许超频，这使得程序员能够比标准Minecraft更快地显示场景，然而Malmo目前只能提供Minecraft游戏环节，相比于OpenAI Universe 支持的环境广度有所不足。\n",
    "\n",
    "\n",
    "- VizDoom是一种基于Doom的AI平台，它支持在多智能体和竞争环境下测试智能体，但是它只能够使用Doom游戏环境。\n",
    "\n",
    "在本课程中，我们在这只关注OpenAI Gym，有兴趣的同学可以关注其他平台。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62a0e6",
   "metadata": {},
   "source": [
    "## 5.1 OpenAI Gym简介\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c657ed22",
   "metadata": {},
   "source": [
    "2015年，特斯拉CEO Elon Musk，LinkedIn创始人Reid Hoffman及PayPal创始人Peter Thiel等人共同宣布创立非盈利的研究公司OpenAI。\n",
    "\n",
    "OpenAI是地球上最强的人工智能研究团队之一，致力于进行非监督式学习和强化学习的研究。OpenAI的使命和长期目标是以最大限度地造福全人类的方式发展人工智能。OpenAI Gym 是一个用于开发和比较RL算法的工具包，与其他的数值计算库兼容，如tensorflow 或者theano 库。现在主要支持的是python 语言，以后将支持其他语言。\n",
    "\n",
    "OpenAI Gym包含两部分：\n",
    "\n",
    "+ Gym开源程序包含一个测试问题集，每个问题定义为环境，可以用于自己的强化学习算法开发，这些环境有共享的接口，允许用户设计通用的算法，例如：Atari、CartPole等\n",
    "\n",
    "+ OpenAI Gym服务提供一个站点和API，允许用户对自己训练的算法进行性能比较\n",
    "\n",
    "我们在接下来的分析路径是，首先以单摆车案例来熟悉Gym环境以及强化学习的算法，然后再依据单摆车案例的强化学习逻辑处理金融金融案例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a8bec",
   "metadata": {},
   "source": [
    "## 5.2 Gym安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01d6ca",
   "metadata": {},
   "source": [
    "如果没有安装gym需要安装\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c198cd5",
   "metadata": {},
   "source": [
    "或在Anaconda命令窗口中执行以下命令：\n",
    "\n",
    "git clone https://github.com/openai/gym  </p>\n",
    "cd gym  \n",
    "pip install -e .[all]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6457871",
   "metadata": {},
   "source": [
    "## 5.3 实验：了解Gym仿真环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3df6e2",
   "metadata": {},
   "source": [
    "【实验目的】\n",
    "\n",
    "+ 了解OpenAI Gym仿真环境\n",
    "\n",
    "+ 了解OpenAI Gym仿真环境的模拟案例\n",
    "\n",
    "+ 了解Gym源文件的查看方法以及源文件的内容\n",
    "\n",
    "【实验步骤】\n",
    "\n",
    "+ 键入代码，实现相同返回结果\n",
    "\n",
    "+ 浏览网站，对Gym环境有概括性了解\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ed2e4",
   "metadata": {},
   "source": [
    "查看仿真环境列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c81ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CartPole-v0', EnvSpec(id='CartPole-v0', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=195.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=0)), ('CartPole-v1', EnvSpec(id='CartPole-v1', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=1)), ('MountainCar-v0', EnvSpec(id='MountainCar-v0', entry_point='gym.envs.classic_control.mountain_car:MountainCarEnv', reward_threshold=-110.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCar', version=0)), ('MountainCarContinuous-v0', EnvSpec(id='MountainCarContinuous-v0', entry_point='gym.envs.classic_control.continuous_mountain_car:Continuous_MountainCarEnv', reward_threshold=90.0, nondeterministic=False, max_episode_steps=999, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCarContinuous', version=0)), ('Pendulum-v1', EnvSpec(id='Pendulum-v1', entry_point='gym.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pendulum', version=1)), ('Acrobot-v1', EnvSpec(id='Acrobot-v1', entry_point='gym.envs.classic_control.acrobot:AcrobotEnv', reward_threshold=-100.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Acrobot', version=1)), ('LunarLander-v2', EnvSpec(id='LunarLander-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='LunarLander', version=2)), ('LunarLanderContinuous-v2', EnvSpec(id='LunarLanderContinuous-v2', entry_point='gym.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'continuous': True}, namespace=None, name='LunarLanderContinuous', version=2)), ('BipedalWalker-v3', EnvSpec(id='BipedalWalker-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=1600, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='BipedalWalker', version=3)), ('BipedalWalkerHardcore-v3', EnvSpec(id='BipedalWalkerHardcore-v3', entry_point='gym.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=2000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'hardcore': True}, namespace=None, name='BipedalWalkerHardcore', version=3)), ('CarRacing-v2', EnvSpec(id='CarRacing-v2', entry_point='gym.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CarRacing', version=2)), ('Blackjack-v1', EnvSpec(id='Blackjack-v1', entry_point='gym.envs.toy_text.blackjack:BlackjackEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'sab': True, 'natural': False}, namespace=None, name='Blackjack', version=1)), ('FrozenLake-v1', EnvSpec(id='FrozenLake-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.7, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '4x4'}, namespace=None, name='FrozenLake', version=1)), ('FrozenLake8x8-v1', EnvSpec(id='FrozenLake8x8-v1', entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.85, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '8x8'}, namespace=None, name='FrozenLake8x8', version=1)), ('CliffWalking-v0', EnvSpec(id='CliffWalking-v0', entry_point='gym.envs.toy_text.cliffwalking:CliffWalkingEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CliffWalking', version=0)), ('Taxi-v3', EnvSpec(id='Taxi-v3', entry_point='gym.envs.toy_text.taxi:TaxiEnv', reward_threshold=8, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Taxi', version=3)), ('Reacher-v2', EnvSpec(id='Reacher-v2', entry_point='gym.envs.mujoco:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=2)), ('Reacher-v4', EnvSpec(id='Reacher-v4', entry_point='gym.envs.mujoco.reacher_v4:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=4)), ('Pusher-v2', EnvSpec(id='Pusher-v2', entry_point='gym.envs.mujoco:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=2)), ('Pusher-v4', EnvSpec(id='Pusher-v4', entry_point='gym.envs.mujoco.pusher_v4:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=4)), ('InvertedPendulum-v2', EnvSpec(id='InvertedPendulum-v2', entry_point='gym.envs.mujoco:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=2)), ('InvertedPendulum-v4', EnvSpec(id='InvertedPendulum-v4', entry_point='gym.envs.mujoco.inverted_pendulum_v4:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=4)), ('InvertedDoublePendulum-v2', EnvSpec(id='InvertedDoublePendulum-v2', entry_point='gym.envs.mujoco:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=2)), ('InvertedDoublePendulum-v4', EnvSpec(id='InvertedDoublePendulum-v4', entry_point='gym.envs.mujoco.inverted_double_pendulum_v4:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=4)), ('HalfCheetah-v2', EnvSpec(id='HalfCheetah-v2', entry_point='gym.envs.mujoco:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=2)), ('HalfCheetah-v3', EnvSpec(id='HalfCheetah-v3', entry_point='gym.envs.mujoco.half_cheetah_v3:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=3)), ('HalfCheetah-v4', EnvSpec(id='HalfCheetah-v4', entry_point='gym.envs.mujoco.half_cheetah_v4:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=4)), ('Hopper-v2', EnvSpec(id='Hopper-v2', entry_point='gym.envs.mujoco:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=2)), ('Hopper-v3', EnvSpec(id='Hopper-v3', entry_point='gym.envs.mujoco.hopper_v3:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=3)), ('Hopper-v4', EnvSpec(id='Hopper-v4', entry_point='gym.envs.mujoco.hopper_v4:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=4)), ('Swimmer-v2', EnvSpec(id='Swimmer-v2', entry_point='gym.envs.mujoco:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=2)), ('Swimmer-v3', EnvSpec(id='Swimmer-v3', entry_point='gym.envs.mujoco.swimmer_v3:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=3)), ('Swimmer-v4', EnvSpec(id='Swimmer-v4', entry_point='gym.envs.mujoco.swimmer_v4:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=4)), ('Walker2d-v2', EnvSpec(id='Walker2d-v2', entry_point='gym.envs.mujoco:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=2)), ('Walker2d-v3', EnvSpec(id='Walker2d-v3', entry_point='gym.envs.mujoco.walker2d_v3:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=3)), ('Walker2d-v4', EnvSpec(id='Walker2d-v4', entry_point='gym.envs.mujoco.walker2d_v4:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=4)), ('Ant-v2', EnvSpec(id='Ant-v2', entry_point='gym.envs.mujoco:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=2)), ('Ant-v3', EnvSpec(id='Ant-v3', entry_point='gym.envs.mujoco.ant_v3:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=3)), ('Ant-v4', EnvSpec(id='Ant-v4', entry_point='gym.envs.mujoco.ant_v4:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=4)), ('Humanoid-v2', EnvSpec(id='Humanoid-v2', entry_point='gym.envs.mujoco:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=2)), ('Humanoid-v3', EnvSpec(id='Humanoid-v3', entry_point='gym.envs.mujoco.humanoid_v3:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=3)), ('Humanoid-v4', EnvSpec(id='Humanoid-v4', entry_point='gym.envs.mujoco.humanoid_v4:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=4)), ('HumanoidStandup-v2', EnvSpec(id='HumanoidStandup-v2', entry_point='gym.envs.mujoco:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=2)), ('HumanoidStandup-v4', EnvSpec(id='HumanoidStandup-v4', entry_point='gym.envs.mujoco.humanoidstandup_v4:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=4))]\n"
     ]
    }
   ],
   "source": [
    "# 显示所有仿真环境\n",
    "from gym import envs\n",
    "print(list(envs.registry.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935b4a2",
   "metadata": {},
   "source": [
    "如需查看源文件，执行以下命令："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c17470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mie/opt/anaconda3/lib/python3.9/site-packages/gym/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print (gym.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e26c3c",
   "metadata": {},
   "source": [
    "将以上结果拷贝至文件浏览器中，可查看相应的源文件说明。在接下来的部分中会用到此功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b6525",
   "metadata": {},
   "source": [
    "## 5.4. 实验：单摆车问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e86de",
   "metadata": {},
   "source": [
    "### 5.4.1 案例描述\n",
    "\n",
    "【实验概述】\n",
    "\n",
    "单摆车（CartPole）是Gym仿真环境的经典案例。强化学习中的元素在单摆车仿真实验中是这样描述的：\n",
    "\n",
    "+ 状态\n",
    "\n",
    "    状态用四个变量来描述：[$x,\\hat{x},\\theta,\\hat{\\theta}$]\n",
    "    \n",
    "    \n",
    "$x$表示小车在轨道上的位移（Displacement）。注意，我们将位移定义为一个向量，初始位置其值为0，右侧为正，左侧为负。\n",
    "\n",
    "$\\hat{x}$表示小车速度（Cart velocity），右侧为正，左侧为负。\n",
    "\n",
    "$\\theta$表示杆子与竖直方向的夹角（Angle of the pole with the vertical line）。\n",
    "\n",
    "$\\hat{\\theta}$表示角度变化的速率（Rate of change of the angle）。\n",
    "\n",
    "   \n",
    "    \n",
    "+ 动作\n",
    "\n",
    "    动作只有两个0和1。动作0表示向左拉动小车，动作1表示向右拉动小车。每次执行动作0或者1可将小车的位置和单摆偏离角改变，每次改变的结果根据当时小车的状态来确定；当小车状态相同时，每次执行相同的动作得到的结果也相同。\n",
    "    \n",
    "    \n",
    "+ 回报\n",
    "\n",
    "    回报较为简单，即执行一次动作，得到回报+1，没有其他回报值\n",
    "    \n",
    "    \n",
    "+ 目标\n",
    "\n",
    "    将执行动作后所得到的回报进行加总，总回报的最大值是目标，也就是长期累积执行的动作次数\n",
    "    \n",
    "    \n",
    "+ 策略\n",
    "\n",
    "    要得到的策略是将单摆车所处的状态及其所对应的相对动作（0或者1）匹配。也就是在给定状态的情况下，找到对应的策略能够描述应该执行哪个动作\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45c290",
   "metadata": {},
   "source": [
    "这里，我们除了定义强化学习的要素以外，还需要额外定义任务的成功或者失败。我们把一次任务称为一“回合”（episode）,回合可以理解为玩游戏的一局，成功就是通关，失败就是中途结束，无论是成功还是失败，都需要重新再来，开启一个新的回合。对于单摆车实验，成功和失败是这样定义的：\n",
    "\n",
    "成功：\n",
    "+ 执行的动作数目过200时步（累积回报率超过200），或者说在小车上施加动作达到200次\n",
    "\n",
    "失败：\n",
    "\n",
    "只要有一次执行动作后，小车的新状态发生一下情形之一：\n",
    "\n",
    "\n",
    "+ 小车位移$x$大于2.4\n",
    "+ 单摆角度$\\theta$大于$12^\\circ$（弧度制0.2093）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b331b1d",
   "metadata": {},
   "source": [
    "【实验目的】\n",
    "\n",
    "+ 通过Gym实验了解OpenAI Gym仿真环境，了解单摆车工作原理\n",
    "\n",
    "+ 熟悉单摆车基本函数及其功能\n",
    "\n",
    "+ 能够将强化学习的要素与单摆车问题相对应\n",
    "\n",
    "+ 按照单摆车成功与失败条件进行程序模拟\n",
    "\n",
    "【实验要求】\n",
    "\n",
    "+ 仔细阅读题目条件，<font color=red>逐行键入代码</font>\n",
    "+ 键入过程中，如果编译报错，需要仔细阅读错误提示，<font color=red>不能囫囵吞枣直接拷贝代码</font>\n",
    "+ 分析每行代码的输入和输出（如果有）\n",
    "+ 遇到函数关系较为复杂时，注意函数之间的接口，确保接口数据类型一一对应\n",
    "+ 如果有函数功能不清楚，需要单独提取并查询其函数内在逻辑\n",
    "\n",
    "\n",
    "\n",
    "【实验步骤】\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48bf1490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T00:57:36.191286Z",
     "start_time": "2022-02-16T00:57:35.594969Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入gym环境\n",
    "import gym\n",
    "\n",
    "# 关闭所有警告\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 引入画图工具\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('tkagg')\n",
    "\n",
    "# 设置显示精度以及不适用科学计数法显示小数\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# 禁止hash随机化，使得实验可复现\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9cf44df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T00:57:36.329561Z",
     "start_time": "2022-02-16T00:57:36.310002Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建单摆车的仿真环境\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15064335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T00:57:46.248828Z",
     "start_time": "2022-02-16T00:57:46.234107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.0358, -0.0108, -0.0446, -0.0206], dtype=float32), {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重置环境。每次执行env.reset（）函数，环境都将小车的状态进行重置，得到一个新的四元组状态\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce8dba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T00:57:52.581017Z",
     "start_time": "2022-02-16T00:57:47.266545Z"
    }
   },
   "outputs": [],
   "source": [
    "# 环境渲染，产生一个窗口显示单摆车的状态\n",
    "env.render()\n",
    "\n",
    "# 引入时间库，图像显示5秒钟后关闭\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "# 关闭仿真环境，关闭渲染图景\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab8b7935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T00:58:08.673500Z",
     "start_time": "2022-02-16T00:58:08.658759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间： Box([-4.8000e+00 -3.4028e+38 -4.1888e-01 -3.4028e+38], [4.8000e+00 3.4028e+38 4.1888e-01 3.4028e+38], (4,), float32)\n",
      "动作空间： Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# 查看状态空间与动作空间\n",
    "print(\"状态空间：\",env.observation_space)\n",
    "print(\"动作空间：\",env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f285059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T00:58:10.101340Z",
     "start_time": "2022-02-16T00:58:10.090622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8     inf 0.419   inf]\n",
      "[-4.8     -inf -0.419   -inf]\n"
     ]
    }
   ],
   "source": [
    "# 查看状态空间的边界\n",
    "print(env.observation_space.high.astype(np.float16))\n",
    "print(env.observation_space.low.astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05008637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T00:58:12.605019Z",
     "start_time": "2022-02-16T00:58:12.586988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 随机选取动作空间中的10个动作，查看结果\n",
    "for i in range(10):\n",
    "    print (env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0333d93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T00:58:16.990003Z",
     "start_time": "2022-02-16T00:58:16.970181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.6671e-01 -3.8426e+37 -2.0169e-01  3.3093e+38]\n",
      "[-2.5239e+00  1.5328e+38 -1.6035e-01 -8.3699e+37]\n",
      "[ 3.1367e+00 -2.2285e+38  9.8012e-02 -3.1646e+38]\n",
      "[-4.7136e+00  3.1508e+38  3.4457e-01  2.9128e+38]\n",
      "[ 3.9356e+00 -1.5249e+38 -1.7139e-02 -2.1096e+38]\n",
      "[ 4.3358e+00  1.9016e+38 -3.8313e-01  2.9990e+38]\n",
      "[-6.8551e-01 -7.5684e+36  1.4156e-01  3.2791e+38]\n",
      "[-4.1959e-01 -2.5508e+37  1.5879e-01 -9.8104e+37]\n",
      "[-1.5979e+00 -9.4458e+37 -2.0912e-01 -2.4941e+38]\n",
      "[ 3.2023e+00 -2.0173e+38 -4.1834e-01  2.4354e+38]\n"
     ]
    }
   ],
   "source": [
    "# 随机选取十个状态，结果返回状态四元组\n",
    "for i in range(10):\n",
    "    print (env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "051de2ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T00:58:17.847707Z",
     "start_time": "2022-02-16T00:58:17.842541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0024 -0.2169 -0.0494  0.2845]\n",
      "1.0\n",
      "False\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "env.reset()#重置状态\n",
    "observation, reward, done, _, info = env.step(0) # 执行动作0，每次执行动作以后，四元组都会根据状态和所执行的动作返回一个的新状态\n",
    "print (observation)\n",
    "print (reward)\n",
    "print (done)\n",
    "print (info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9f14da5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T00:58:19.828892Z",
     "start_time": "2022-02-16T00:58:19.819251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "执行动作前，状态是： (array([-0.0185,  0.025 ,  0.0068, -0.0402], dtype=float32), {})\n",
      "执行动作后，新的状态是： [-0.018   0.22    0.006  -0.3307]\n",
      "执行动作后，奖励是： 1.0\n",
      "执行动作后，Done是： False\n",
      "执行动作后，info是： {}\n"
     ]
    }
   ],
   "source": [
    "obs=env.reset()\n",
    "\n",
    "print (\"执行动作前，状态是：\",obs)\n",
    "\n",
    "action=1\n",
    "observation, reward, done, truncated, info = env.step(action) # 执行动作0。 \n",
    "                                                 # 执行动作1。每次执行动作1或者0以后，四元组都会根据状态和执行的动作返回一个的新状态，\n",
    "                                                 # 同时给定回报，done和info      \n",
    "print (\"执行动作后，新的状态是：\",observation)\n",
    "print (\"执行动作后，奖励是：\",reward)\n",
    "print (\"执行动作后，Done是：\",done)\n",
    "print (\"执行动作后，info是：\",info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "712e6a0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T00:58:20.887701Z",
     "start_time": "2022-02-16T00:58:20.874342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "执行动作前，状态是： (array([-0.0051, -0.0311,  0.0045,  0.001 ], dtype=float32), {})\n",
      "执行动作后，新的状态是： [-0.0057 -0.2263  0.0045  0.2951]\n",
      "执行动作后，奖励是： 1.0\n",
      "执行动作后，Done是： False\n",
      "执行动作后，info是： {}\n"
     ]
    }
   ],
   "source": [
    "obs=env.reset()\n",
    "print (\"执行动作前，状态是：\",obs)\n",
    "action=0\n",
    "observation, reward, done, truncated, info = env.step(action) # 执行动作0。      \n",
    "print (\"执行动作后，新的状态是：\",observation)\n",
    "print (\"执行动作后，奖励是：\",reward)\n",
    "print (\"执行动作后，Done是：\",done)\n",
    "print (\"执行动作后，info是：\",info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391be24",
   "metadata": {},
   "source": [
    "在这里，我们需要强化一个概念，episode，之前说过，翻译成“回合”较为妥当（也有翻译成“片段”的）。\n",
    "\n",
    "一个回合指的是智能体完成某一任务的一个完整的过程，完成某一任务当然也包含任务失败，提前终止。比如说，玩游戏从开始直至通关，或者中途游戏主角因为犯错所导致的惩罚过高而提前结束游戏；自动驾驶任务中，从始发地到目的地，或者中途出现其他状况导致过程不能继续；做金融交易，完成一年的交易任务（目标为一年）或者一年内由于资金耗尽而不能继续，等等。\n",
    "\n",
    "在单摆车任务中，回合的结束或终止的条件是：\n",
    "\n",
    "- 总回报达到200，回合成功结束\n",
    "\n",
    "- done在某一次执行step()函数后，结果为True，提前终止。这个条件对应上文中的角度大于12度或者位移大于2.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bdfdded",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T01:12:52.482554Z",
     "start_time": "2022-02-16T01:12:52.468747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=  1 | state=[-0.0319  0.1612  0.0038 -0.2853] | action=1 | reward=1.0| done=False\n",
      "step=  2 | state=[-0.0287  0.3562 -0.0019 -0.5768] | action=1 | reward=1.0| done=False\n",
      "step=  3 | state=[-0.0216  0.5514 -0.0134 -0.8701] | action=1 | reward=1.0| done=False\n",
      "step=  4 | state=[-0.0105  0.7467 -0.0308 -1.1669] | action=1 | reward=1.0| done=False\n",
      "step=  5 | state=[ 0.0044  0.9422 -0.0542 -1.4691] | action=1 | reward=1.0| done=False\n",
      "step=  6 | state=[ 0.0232  0.7478 -0.0835 -1.1938] | action=0 | reward=1.0| done=False\n",
      "step=  7 | state=[ 0.0382  0.9439 -0.1074 -1.5115] | action=1 | reward=1.0| done=False\n",
      "step=  8 | state=[ 0.0571  1.1401 -0.1376 -1.8357] | action=1 | reward=1.0| done=False\n",
      "step=  9 | state=[ 0.0799  0.9468 -0.1744 -1.5887] | action=0 | reward=1.0| done=False\n",
      "step= 10 | state=[ 0.0988  0.7541 -0.2061 -1.3551] | action=0 | reward=1.0| done=False\n",
      "step= 11 | state=[ 0.1139  0.5621 -0.2332 -1.1333] | action=0 | reward=1.0| done=True\n",
      "回合失败！\n"
     ]
    }
   ],
   "source": [
    "#编写一个单摆车案例的回合\n",
    "\n",
    "env.reset()  #重置状态\n",
    "\n",
    "for e in range(1, 200):\n",
    "    \n",
    "    a = env.action_space.sample() #随机选取动作\n",
    "    \n",
    "    state, reward, done, _, info = env.step(a) #执行已经选取的动作\n",
    "    \n",
    "    print(f'step={e:3d} | state={state} | action={a} | reward={reward}| done={done}') #打印结果，e:3d表示用3个位置显示e\n",
    "    \n",
    "    #如果出现done==True，回合结束\n",
    "    if done and (e + 1) < 200:                   \n",
    "        print(\"回合失败！\")  \n",
    "        break\n",
    "    if e==199:\n",
    "        print (\"回报达到200，回合完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04941288",
   "metadata": {},
   "source": [
    "可多次执行以上代码，看平均step的值，一般情况下，很少有超过30次的时候。也就是说，不到30次的实验里，我们就会犯错，使得小车不能继续运行下去。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63740af9",
   "metadata": {},
   "source": [
    "接下来，将上述代码扩展一下，加入图景渲染，总共执行20个回合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99d5c457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T01:24:06.461508Z",
     "start_time": "2022-02-16T01:23:58.747220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第  1回合: 在17时步之后，回合结束\n",
      "第  2回合: 在9时步之后，回合结束\n",
      "第  3回合: 在25时步之后，回合结束\n",
      "第  4回合: 在11时步之后，回合结束\n",
      "第  5回合: 在26时步之后，回合结束\n",
      "第  6回合: 在11时步之后，回合结束\n",
      "第  7回合: 在15时步之后，回合结束\n",
      "第  8回合: 在25时步之后，回合结束\n",
      "第  9回合: 在20时步之后，回合结束\n",
      "第 10回合: 在21时步之后，回合结束\n",
      "第 11回合: 在18时步之后，回合结束\n",
      "第 12回合: 在44时步之后，回合结束\n",
      "第 13回合: 在23时步之后，回合结束\n",
      "第 14回合: 在16时步之后，回合结束\n",
      "第 15回合: 在26时步之后，回合结束\n",
      "第 16回合: 在55时步之后，回合结束\n",
      "第 17回合: 在17时步之后，回合结束\n",
      "第 18回合: 在13时步之后，回合结束\n",
      "第 19回合: 在44时步之后，回合结束\n",
      "第 20回合: 在19时步之后，回合结束\n"
     ]
    }
   ],
   "source": [
    "#为了独立运行，重新导入gym库\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "for i_episode in range(20):         #执行20个回合\n",
    "    observation = env.reset()\n",
    "    for t in range(200):\n",
    "        env.render()                 #图形渲染\n",
    "        #print(\"当前状态：\", observation)         \n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _, info = env.step(action)\n",
    "        if done:\n",
    "            print(f\"第{i_episode+1:3d}回合: 在{t+1}时步之后，回合结束\")\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2b2a92",
   "metadata": {},
   "source": [
    "接下来，将随机使用动作应用到单摆车的过程写成自定义函数，将回合内的最后一次循环的次数作为返回值。由于每循环一次总回报加一，所以最后一次循环的次数就是此回合的总回报。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a26947d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T01:25:32.861858Z",
     "start_time": "2022-02-16T01:25:32.843827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每次循环随机指定状态,随机状态下,随机使用动作,多次执行,看在随机选定动作下,平均回报是多少\n",
    "\n",
    "def run_episode():\n",
    "    state=env.reset()\n",
    "    for e in range(1, 201):\n",
    "        \n",
    "        a = env.action_space.sample()\n",
    "        \n",
    "        state, reward, done, _, info = env.step(a) \n",
    "                       \n",
    "        if done and (e+1)< 200:\n",
    "            break\n",
    "    return e\n",
    "\n",
    "run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3313f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T01:25:39.066176Z",
     "start_time": "2022-02-16T01:25:33.547558Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加入渲染效果的run_episode()\n",
    "\n",
    "def run_episode_visulization():\n",
    "    import time \n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    for e in range(1, 201):\n",
    "        a = env.action_space.sample()\n",
    "        state, reward, done, _, info = env.step(a) \n",
    "        #状态更新后，开始渲染\n",
    "        env.render()\n",
    "        #每次渲染持续0.2秒后进行下一次\n",
    "        time.sleep(0.5)\n",
    "        #如果done或者达到200次，则退出\n",
    "        if done and e < 200:\n",
    "            env.close()\n",
    "            break\n",
    "        \n",
    "    return e\n",
    "\n",
    "run_episode_visulization()#运行可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3439b346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T01:26:16.407254Z",
     "start_time": "2022-02-16T01:26:16.011506Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#%%time的目的是记录单元格运行时间，注意%%为魔法函数必须放在单元格开始位置\n",
    "\n",
    "#创建列表用于存储每一回合的回报\n",
    "reward=[]\n",
    "\n",
    "# 回合执行1000次，将每一次回报存入reward列表\n",
    "for i in range(1,1001):\n",
    "    reward.append(run_episode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9478a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T01:26:17.789842Z",
     "start_time": "2022-02-16T01:26:17.594831Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将reward列表打印出来\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] #解决中文显示乱码问题，也可使用其他字体\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(reward)\n",
    "plt.ylabel(\"总回报\")\n",
    "plt.xlabel(\"回合次数\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352b767",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T01:26:26.496486Z",
     "start_time": "2022-02-16T01:26:26.482590Z"
    }
   },
   "outputs": [],
   "source": [
    "print (\"平均回报为：\",np.mean(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d860f26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T01:26:27.356538Z",
     "start_time": "2022-02-16T01:26:27.339191Z"
    }
   },
   "outputs": [],
   "source": [
    "print (\"最大回报为：\",np.max(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a496fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T01:26:28.157749Z",
     "start_time": "2022-02-16T01:26:28.139864Z"
    }
   },
   "outputs": [],
   "source": [
    "print (\"最小回报为：\",np.min(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981860c",
   "metadata": {},
   "source": [
    "### 5.4.2 实验总结\n",
    "\n",
    "从以上代码中也可以看出，Gym的核心接口是Env(环境)。作为统一的环境接口，Env包含下面几个核心方法：\n",
    "\n",
    "+ <font color=#FF9900>env.reset(self)</font>：重置状态，返回新状态（观测值）\n",
    "+ <font color=#FF9900>env.step(self, action)</font>：推进一个时间步长，返回 <font color=#FF9900>observation, reward, done, info</font> 四元组。当当前状态确定的情况下，每次执行相同的动作返回值相同，此假设是环境与动作交互的基础\n",
    "+ <font color=#FF9900>env.render(self)</font>：弹出一个新的窗口，重绘环境的一帧\n",
    "+ <font color=#FF9900>env.close(self)</font>：关闭环境，并清除内存\n",
    "\n",
    "\n",
    "在上面代码中使用了<font color=#FF9900>env.step()</font>函数来对每一步进行仿真，在Gym中，<font color=#FF9900>env.step()</font>会返回 4 个参数：\n",
    "\n",
    "+ 观测<font color=#FF9900>Observation (Object)</font>：当前step执行后，环境的观测(类型为对象)\n",
    "\n",
    "+ 奖励<font color=#FF9900>Reward (Float)</font>: 执行上一步动作(action)后，智能体(agent)获得的奖励(浮点类型)，不同的环境中奖励值变化范围也不相同，但是强化学习的目标就是使得总奖励值最大\n",
    "\n",
    "+ 完成<font color=#FF9900>Done (Boolean)</font>: 表示回合是否已经终止。当<font color=#FF9900>Done</font>为True 时，就表明当前回合终止\n",
    "\n",
    "+ 信息<font color=#FF9900>Info (Dict)</font>: 可以记录调试过程的诊断信息。在单摆车案例中，不会使用到这个info，始终为空值\n",
    "\n",
    "+ 在 Gym 仿真中，每一次回合开始，需要先执行<font color=#FF9900>reset()</font>函数，返回初始观测信息，然后根据标记<font color=#FF9900>Done</font>的状态，来决定是否进行下一次回合\n",
    "\n",
    "<font color=#FF9900>step()</font>函数非常重要，可以翻译成时步函数(time step function)，他表示我们让系统“向前”走“一步”、“一个时间间隔”或“一帧”等，也就是在时间上向前进行最小的一个单位。\n",
    "\n",
    "注意：\n",
    "以上代码执行step函数，返回的也是四个数字的列表，这个列表与状态不同，分别为状态，回报，Done，Info。其中，状态自身为四元组列表，内部结构为： $[x,\\hat{x},\\theta,\\hat{\\theta}]$; 回报表示当前执行step函数以后的回报，每一次执行一次回报为1(在优化整体策略时，我们需要额外设置全局变量总回报，将每一次得到的回报进行加总)； Done表示当前回合是否结束，每一次执行step()都会有所变化，Done=False时，小车还可继续运行，Done=True时，回合失败结束；不要将step函数返回的四元组与状态本身的四元组混淆。\n",
    "\n",
    "以上实验中可以看出，在随机选择动作的情况下，小车很难达到回合胜利结束（回报达到200），平均回报仅为22左右。我们的目标是，经过强化学习的训练，智能体能够知道小车在什么状态下执行什么动作，最终使得小车的综合平均回报能够达到或大于200。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbc0e5",
   "metadata": {},
   "source": [
    "## 5.5. 实验：利用爬山算法进行强化学习\n",
    "\n",
    "本实验将改变随机选择动作的策略，系统性地选择优化动作来实现完成回合的目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd1669",
   "metadata": {},
   "source": [
    "### 5.5.1 爬山算法（Hill Climbing）\n",
    "\n",
    "【实验概述】\n",
    "\n",
    "处理单摆车的时候我们用到的状态使用四个变量来衡量的，即，小车的位移，速度，单摆的角度，单摆的角速度。四个变量在实际估计时，处理会相对复杂。为了简化问题，我们可以创建一个新的四维向量，叫做权值，也叫权向量。使权值与原有的状态四元组做向量的点乘积，这样得到一个一维数字，作为新的状态变量，这个一维状态变量在利用爬山法学习时有很大便利性。\n",
    "\n",
    "爬山算法属于我们在第四部分中介绍的蒙特卡罗算法中的一种。它是一种局部择优的启发式方法，是对深度优先搜索的一种改进。爬山算法从当前节点开始和周围的邻居节点的值进行比较。 如果当前节点是最大的，那么返回当前节点，作为最大值(即山峰最高点)；反之就用最高的邻居节点来替换当前节点，从而实现向山峰的高处攀爬的效果。如此循环，直到遍历周边所有的点，达到最高的点。这个算法的优点是避免遍历全局，通过启发选择部分节点，从而达到提高效率的目的。缺点是因为不是全局搜索，所以结果可能不是最佳,也就是说有可能只能找到局部最优值而不是全局最优值。\n",
    "\n",
    "【实验目的】\n",
    "\n",
    "+ 创建新的四维权值,做原状态变量和新权值的点乘积,得到一维变量以简化状态描述\n",
    "\n",
    "+ 根据得到的新的一维变量指定相应的动作：\n",
    "\n",
    "    如一维状态变量s>=0, 则动作action=1； 如一维状态s<0, 动作action=0。以下简称“零一规则”\n",
    "    \n",
    "    \n",
    "+ 通过爬山算法进行多回合实验进行优化,最终确定在“零一规则”中的最优权值\n",
    "\n",
    "+ 将此权值应用于“零一规则”施加于单摆车, 最终达到每一回合高概率达到总回报200,从而完成单摆车任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4c867",
   "metadata": {},
   "source": [
    "### 5.5.2 实验：爬山算法的实现\n",
    "\n",
    "【实验步骤】\n",
    "\n",
    "+ 逐行键入以下代码，观察输出，确保输出和示例一致\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e972c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T03:28:36.626676Z",
     "start_time": "2022-02-16T03:28:36.621569Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import numpy as np \n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d5de9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T03:28:37.165910Z",
     "start_time": "2022-02-16T03:28:37.148854Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设定种子值，以确保以下产生的随机种子确定\n",
    "seed=100\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 定义一个权值向量\n",
    "weights = np.random.random(4) * 2 - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50676f62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T03:28:37.459783Z",
     "start_time": "2022-02-16T03:28:37.444264Z"
    }
   },
   "outputs": [],
   "source": [
    "# 此处需要注意随机种子的使用特征：与上一个单元格做对比，再次使用相同的随机数，需要再次确定随机种子\n",
    "\n",
    "weights = np.random.random(4) * 2 - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c66a5f",
   "metadata": {},
   "source": [
    "请同学们往复执行上两个单元格，可以看到不同的结果！\n",
    "\n",
    "接下来重新建立仿真环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a272b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T03:28:38.628690Z",
     "start_time": "2022-02-16T03:28:38.611199Z"
    }
   },
   "outputs": [],
   "source": [
    "# 建立单摆车仿真环境\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# 设置仿真环境中的种子值，确保以下产生的环境变量每次确定\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32854f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T03:28:39.427412Z",
     "start_time": "2022-02-16T03:28:39.420724Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "a=random.choice([0,1])\n",
    "\n",
    "#执行时步函数\n",
    "state, reward, done, _, info = env.step(a) \n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065265f4",
   "metadata": {},
   "source": [
    "同样，可以往复执行上两个单元格，或只执行上一个单元格，观察区别。四维权值和四维状态已经确定，可以做点乘积，得到一维状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4332d057",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T03:28:42.265151Z",
     "start_time": "2022-02-16T03:28:42.260518Z"
    }
   },
   "outputs": [],
   "source": [
    "# 权值向量乘以初始状态，得到一个一维的数据，表示状态，这是一种降维的典型方法\n",
    "s = np.dot(state, weights)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bce68f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T03:28:42.849771Z",
     "start_time": "2022-02-16T03:28:42.844156Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义动作，如果一维状态<0，则动作为0,否则动作为1；注意此处的python语句\n",
    "a = 0 if s<0 else 1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1cf7a9",
   "metadata": {},
   "source": [
    "到此为止，我们完成了一个简单的降维方法：将小车状态的四元组变量乘以一个权值，将其装换为一个一维状态变量。并且定义了这个一维状态变量所对应的动作。接下来进入爬山法学习过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812c0fb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T03:40:06.717216Z",
     "start_time": "2022-02-16T03:40:06.709952Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个执行一个回合的函数。输入为环境和权值，返回总回报\n",
    "# 此函数执行之前首先利用np.random.seed指定了固定的随机种子，所以在权值和状态都固定的情况下，此函数返回总回报\n",
    "\n",
    "\n",
    "def run_episode2(env, weights):  # 注意：自定义函数run_episode2传递的第一个参数为env,env需要在之前事先定义好，\n",
    "                                 # 为env = gym.make('CartPole-v0')，目的是为了接下来调用env.reset()使用\n",
    "    \n",
    "    state = env.reset()[0]\n",
    "    \n",
    "    #定义总回报\n",
    "    treward = 0\n",
    "    \n",
    "    for _ in range(200):\n",
    "        \n",
    "        #状态降维\n",
    "        s = np.dot(state, weights)\n",
    "        \n",
    "        #根据降维后的状态选择对应的动作\n",
    "        a = 0 if s < 0 else 1\n",
    "        \n",
    "        state, reward, done, _, info = env.step(a)\n",
    "        \n",
    "        treward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    return treward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff82f07",
   "metadata": {},
   "source": [
    "<font color=#FF9900>run_episode2()</font>的目的是：在给定权重的情况下，通过“零一规则”返回一个回合中此权重的总回报。接下来我们可以穷举很多权重，分别带入<font color=#FF9900>run_episode2()</font>，将最优权重保留下来。由于<font color=#FF9900>run_episode2()</font>能够运行的回合数目是有限的，所以此算法是局部最优，非全局最优。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18033742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:00:13.639538Z",
     "start_time": "2022-02-16T07:00:13.616076Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置三个环境中的随机种子函数\n",
    "def set_seeds(seed=100):\n",
    "    import random\n",
    "    import numpy as np\n",
    "        \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.action_space.seed(seed)\n",
    "\n",
    "# 设置三个环境中的随机种子\n",
    "set_seeds()\n",
    "\n",
    "# 定义回合执行次数\n",
    "num_episodes = 1000\n",
    "\n",
    "\n",
    "# 首先记录besttreward(best total reward)=0\n",
    "# 接下来迭代的过程中首先指定权值，调用run_episode2计算一维状态，并根据一维状态变量持续更新状态，加总回报，返回回报\n",
    "besttreward = 0\n",
    "\n",
    "\n",
    "# 学习num_episodes次或进行num_episodes个回合。\n",
    "# 注意python左闭右开区间，总数为num_episodes\n",
    "for e in range(1, num_episodes + 1):\n",
    "\n",
    "    weights = np.random.rand(4) * 2 - 1 #注意此位置，由于之前的np.random.seed(100)已经产生过一次随机数了，这个随机种子只管一次使用\n",
    "                                        #再次使用相同的随机数，需要再次确定随机种子，所以此位置每次显示的是不同的权值\n",
    "    \n",
    "    treward = run_episode2(env, weights) #运行run_episode2函数（环境，权值），返回在此环境和权值给定的情况下的回报\n",
    "\n",
    "    if treward > besttreward: #如果产生回报大于当前最好回报，则更新最好回报和对应的权值\n",
    "        besttreward = treward\n",
    "        bestweights = weights\n",
    "        if treward == 200:\n",
    "            print(f'成功  | episode={e}')\n",
    "            break\n",
    "        print(f'更新  | episode={e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2064d1c",
   "metadata": {},
   "source": [
    "接下来，我们分步解释一下爬山法的学习过程:\n",
    "\n",
    "+ 首先，在<font color=#FF9900>run_episode2()</font>函数中, 给定的权值是不变的，权值作为输入传递到函数内部。在函数内部状态改变，是通过运行<font color=#FF9900>env.step()</font>函数完成的\n",
    "\n",
    "+ 其次，在爬山算法代码中（以上单元格），进入循环（每一回合）以后，每次都随机指定权值weights=np.random.rand(4) * 2 - 1\n",
    "\n",
    "+ 权值点乘当前状态得到新的降维后的状态\n",
    "\n",
    "+ 经过对比以后，将对应的动作0或者1输入给系统，系统返回给<font color=#FF9900>run_episode2()</font>函数此环境下和此权值下的总回报treward \n",
    "\n",
    "+ 经过多次迭代（num_episodes或称多个回合）， 如果哪一个权值能够在随机状态的情况下达到执行动作200次，即回报为200，这一权值胜出并得以保留在bestweights，其余之前每次迭代的权值都被丢弃\n",
    "\n",
    "+ 最优权值bestweights可以使小车运行200次，即完成了任务\n",
    "\n",
    "+ 这个最优权值就是爬山法学习的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008554d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T03:41:19.637984Z",
     "start_time": "2022-02-16T03:41:19.627756Z"
    }
   },
   "outputs": [],
   "source": [
    "print (\"最佳权值：\",bestweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a13b03c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T03:42:13.615793Z",
     "start_time": "2022-02-16T03:41:46.293370Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 查看训练后结果\n",
    "\n",
    "#创建结果列表\n",
    "res = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    treward = run_episode2(env, bestweights) #使用bestweights，随机运行10000次\n",
    "    res.append(treward)\n",
    "    \n",
    "res[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158f556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T06:51:58.539790Z",
     "start_time": "2022-02-16T06:51:58.399949Z"
    }
   },
   "outputs": [],
   "source": [
    "# 查看各个回合的回报\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] #解决中文显示乱码问题，也可使用其他字体\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.ylim(0,300)\n",
    "plt.plot(res)\n",
    "plt.ylabel(\"总回报\")\n",
    "plt.xlabel(\"回合次数\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算平均结果\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57297a42",
   "metadata": {},
   "source": [
    "### 5.5.3 实验总结\n",
    "\n",
    "综上所述，我们利用权值的修改，经过多次迭代确定了在给定环境下的最优权值，并利用了最优权值进行了再次实验。实验的结果每一次都可以成功。这就是我们想要的结果。需要指出的是，此权值与动作的对应关系（零一规则）与迷宫实验的Q表有着异曲同工之处，他们都指导了智能体的在给定状态下做哪一个固定的动作，这个动作已经被验证过，有最大的回报价值。\n",
    "\n",
    "对于爬山算法的实现能够达到近乎完美的效果，在实务中并不多见。究其原因，施加于小车的动作及小车状态的变化完全遵循物理规律。这个物理规律可以用函数式表达（具体规则查看：<font color=#FF9900>C:\\Users\\用户名\\anaconda3\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py</font>）。但是在社会学实践中，往往结果不会这么完美。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b9e5f",
   "metadata": {},
   "source": [
    "总结一下，第五部分给我们提供了两方面的重要启示：\n",
    "\n",
    "一方面，在一个未知环境中，环境可以用一个或多个变量来描述。这些变量大多情况是一个连续型变量，理论上，它有无数种取值（就像单摆车的状态变量），也就是说我们没有办法准确地列出所有状态的列表。\n",
    "\n",
    "另一方面，我们能够执行的动作相对较少，向左0或者向右1，我们想知道在什么样的环境下执行哪个动作是最优的。\n",
    "\n",
    "类似于此类问题，状态无限而动作有限的问题，可以通过爬山法（蒙特卡洛模拟法）解决。我们在给定“零一规则”，a = 0 if s < 0 else 1 的情况下，确定了最优权值bestweights，需要指出的是，此最优权值不是唯一的。同学们可以试着将<font color=#FF9900>set_seeds()</font>（注释“设置三个环境中的随机种子”之下的 set_seeds()）注释掉，比对当前结果。注意，<font color=#FF9900>set_seeds()</font>函数能够控制每次随机选择是确定值还是随机值，且仅对下一次执行有效。"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "无",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
