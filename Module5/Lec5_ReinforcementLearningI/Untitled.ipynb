{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6bd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05870929",
   "metadata": {},
   "source": [
    "### Learning Agents\n",
    "\n",
    "- The agent repeatedly takes **action** $a_t \\in \\mathbb{A}$ in discrete time periods, $t \\in \\mathbb{N}.$\n",
    "- When the agent choose action $a_t$ at time $t$, it obtaints an immediate **observable reward**, $r_{t+1}$.\n",
    "- The **return** $G_t$ at time $t$ is some function $f$ of the future rewards $G_t= f(r_{t+1} , r_{t+2},\\cdots)$.\n",
    "- Often we use $G_t = \\sum_{i=t}^{\\infty} \\gamma^i r_{i+1}$, where $\\gamma \\ in [0,1]$ is the **time discounting** of the agent.\n",
    "\n",
    "### Markov Decision Processes\n",
    "- The reward is a function of the action chosen in the previous state $s_t \\in \\mathbb{S}$.\n",
    "- The $s$ state and action $\\alpha$ at time $t$ determines the probability of the subsequent state s ′ and reward $r$.\n",
    "- The probabilities are specified by the function $p(s',r|s,\\alpha)$.\n",
    "- This specifies a finite **Markov Decision-Process**.\n",
    "- The goal of the agent is to maximise the expected return $\\mathbb{E}(G)$.\n",
    "- The agent follows a **policy** which specifies an action to take in each state: $\\pi(s)\\in\\mathbb{A}$.\n",
    "- The optimal policy is denoted $\\pi^*$ .\n",
    "\n",
    "### Value Functions\n",
    "\n",
    "We use the function $v(\\alpha)$ to denote the expected return for action a over the entire episode:\n",
    "\n",
    "$$v(\\alpha) = \\mathbb{E}(G|a_t = \\alpha)$$\n",
    "\n",
    "$$ = \\mathbb{E}(R_{\\alpha})$$\n",
    "\n",
    "- Typically, the function $v$ is unknown to the agent.\n",
    "- In this scenario, the agent performs *sequential decision making under uncertainty*.\n",
    "\n",
    "Consider a $n$-armed bandit, where $R_{\\alpha}\\sim N(\\alpha, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1cf0dc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0635084643199402"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def play_bandit(a, variance=1.0):\n",
    "    \"\"\" Return the reward from taking action a \"\"\" \n",
    "    return np.random.normal(a, scale=np.sqrt(variance))\n",
    "\n",
    "r_2 = play_bandit(a=1) \n",
    "r_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dac39c",
   "metadata": {},
   "source": [
    "### Greedy Action\n",
    "\n",
    "If we can compute $v(\\alpha)$, then the agent's optimal policy is simple. That is the agent simply choose the action with the highest expection.\n",
    "\n",
    "$$\\alpha^* = arg\\max_{\\alpha} v(\\alpha)$$\n",
    "\n",
    "- we call $\\alpha^*$ as the **greedy action**.\n",
    "\n",
    "### Learning as Sampling\n",
    "\n",
    "The random variates, $r_t$, are directly observable.\n",
    "They are samples from the distribution $F_{\\alpha}(r)$.\n",
    "\n",
    "$$r_t \\sim^{distribution} F_{\\alpha}(r)$$\n",
    "\n",
    "How can we estimate $v(\\alpha)$ given the observed rewards $r_1,r_2, ..., r_t$?\n",
    "\n",
    "### Value Estimation\n",
    "We can use the *sampling to estimate* $v(.)$,\n",
    "\n",
    "$$Q_r(\\alpha) = \\frac{r_1+r_2+...+r_k}{k}$$\n",
    "\n",
    "- By the law of large numbers,\n",
    "\n",
    "$$ \\lim_{k\\to\\infty}Q(\\alpha) = v(\\alpha) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddbbc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_bandit(a, k):\n",
    "    rewards = []\n",
    "    for t in range(k):\n",
    "        r = play_bandit(a)\n",
    "        rewards.append(r)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad2eda9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.560533626413457"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_2 = sample_from_bandit(a=2, k=20)\n",
    "q_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f8d10",
   "metadata": {},
   "source": [
    "#### Optimise the Coding\n",
    "\n",
    "Using `map`, instead of a loop, to speed up the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d847764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_bandit(a,k):\n",
    "    return np.mean(map(lambda i: play_bandita), range(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304c5fb",
   "metadata": {},
   "source": [
    "Or, in a comprehension way,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e49c1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_bandit(a,k):\n",
    "    return np.mean(map(lambda i: play_bandita), range(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031e6b53",
   "metadata": {},
   "source": [
    "Further code optimisation could be implemented, for instance, for large sample sizes we need to allocate memory to hold all the previous samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6395b34e",
   "metadata": {},
   "source": [
    "### Incremental update of estimates\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{split}\n",
    "Q_{k+1} & = \\frac{1}{k} \\sum_{i=1}^k r_i \\\\\n",
    "& = \\frac{1}{k}(r_k + \\sum_{i=1}^{k-1} r_i) \\\\\n",
    "& = \\frac{1}{k}(r_k + (k-1) \\frac{1}{k-1}\\sum_{i=1}^{k-1} r_i) \\\\\n",
    "& = \\frac{1}{k}[r_k + (k-1)Q_k] \\\\\n",
    "& = \\frac{1}{k}[r_k + k\\cdot Q_k - Q_k] \\\\\n",
    "& = Q_k + \\frac{1}{k}[r_k - Q_k] \n",
    "\\end{split}\n",
    "\\end{equation}$$\n",
    "\n",
    "### Temporal Difference Learning\n",
    "- We are adjusting/updating an old estimate towards a new estimate based on more recent information.\n",
    "- We can think of the coefficient $(k)^{−1}$ as a step size parameter.\n",
    "$$Q_{k+1} = \\frac{1}{k} [r_k - Q_k] $$\n",
    "\n",
    "`new_estimate = old_estimate + step_size * (target - old_estimate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cab8305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(old_estimate, target_estimate, k): \n",
    "    step_size = 1./(k+1) \n",
    "    error = target_estimate - old_estimate \n",
    "    return old_estimate + step_size * error\n",
    "\n",
    "def sample_from_bandit(a, k):\n",
    "\n",
    "    current_estimate = 0.\n",
    "    for t in range(k):\n",
    "        current_estimate = update_q(current_estimate, play_bandit(a), t) \n",
    "        return current_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90547da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7672853726906987"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_2 = sample_from_bandit(a=2, k=100000) \n",
    "q_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
